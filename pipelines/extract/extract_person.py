from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

from bs4 import BeautifulSoup
import pandas as pd
import time, os, re, csv
from prefect import task

# ==========================
# ğŸ§  STEP 1: Khá»Ÿi táº¡o Driver
# ==========================
def init_driver(headless=False, window_size="1366,768"):
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
        opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument(f"--window-size={window_size}")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    return driver


# ==========================
# ğŸ§  STEP 2: CÃ o link Wikipedia qua Google
# ==========================
def find_wikipedia_links(topic: str, driver) -> list:
    wait = WebDriverWait(driver, 10)
    query = topic.replace(" ", "+")
    search_url = f"https://www.google.com/search?q={query}+wikipedia"
    driver.get(search_url)
    time.sleep(1)

    # ÄÃ³ng há»™p thoáº¡i cháº¥p nháº­n cookie náº¿u cÃ³
    try:
        consent_button = wait.until(EC.element_to_be_clickable((
            By.XPATH,
            "//button//*[contains(text(),'I agree') or contains(text(),'TÃ´i') or contains(text(),'Cháº¥p nháº­n')]/.."
        )))
        consent_button.click()
        time.sleep(1)
    except Exception:
        pass

    elems = driver.find_elements(By.XPATH, "//a[@href]")
    hrefs = [e.get_attribute("href") for e in elems if e.get_attribute("href")]

    wiki_links = []
    for h in hrefs:
        if "wikipedia.org/wiki/" in h:
            if "/url?q=" in h:
                h = h.split("/url?q=")[1].split("&")[0]
            if h.startswith("http"):
                wiki_links.append(h)
    # loáº¡i trÃ¹ng
    wiki_links = list(dict.fromkeys(wiki_links))
    return wiki_links


# ==========================
# ğŸ§  STEP 3: CÃ o ná»™i dung trang Wikipedia
# ==========================
def scrape_wikipedia(url, driver):
    driver.get(url)
    time.sleep(1.5)
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    title_tag = soup.find("span", class_="mw-page-title-main")
    name = title_tag.text.strip() if title_tag else ""

    birth, death, hometown = None, None, None
    infobox = soup.find("table", class_="infobox")
    if infobox:
        for tr in infobox.find_all("tr"):
            th, td = tr.find("th"), tr.find("td")
            if not th or not td:
                continue
            key = th.get_text(strip=True).lower()
            val = td.get_text(" ", strip=True)
            if "born" in key or "sinh" in key:
                birth = val
            elif "died" in key or "máº¥t" in key:
                death = val
            elif "quÃª" in key or "hometown" in key:
                hometown = val

    content_div = soup.find("div", id="mw-content-text")
    bio = ""
    if content_div:
        paragraphs = content_div.find_all(["p", "ul", "ol", "li", "blockquote"])
        texts = [p.get_text(" ", strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 0]
        bio = "\n\n".join(texts)
    bio = re.sub(r"\[\d+\]", "", bio)
    bio = re.sub(r"\s+", " ", bio).strip()

    return {
        "person_name": name,
        "birth_year": birth,
        "death_year": death,
        "birthplace": hometown,
        "biography": bio,
        "url": url
    }


# ==========================
# ğŸ§  STEP 4: Task Prefect - Extract toÃ n bá»™
# ==========================
@task
def extract_people_data(topics: list, output_path="data/person_raw.csv"):
    driver = init_driver(headless=False)
    os.makedirs("data", exist_ok=True)

    all_data = []
    for topic in topics:
        print(f"\nğŸ” Äang tÃ¬m Wikipedia cho: {topic}")
        wiki_links = find_wikipedia_links(topic, driver)
        if not wiki_links:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y Wikipedia cho {topic}")
            continue

        for link in wiki_links[:1]:  # chá»‰ láº¥y link Ä‘áº§u tiÃªn cho gá»n
            print(f"ğŸ“˜ Äang cÃ o: {link}")
            try:
                data = scrape_wikipedia(link, driver)
                all_data.append(data)
            except Exception as e:
                print(f"âŒ Lá»—i khi cÃ o {link}: {e}")

    pd.DataFrame(all_data).to_csv(output_path, index=False, encoding="utf-8-sig")
    driver.quit()
    print(f"\nâœ… ÄÃ£ lÆ°u {len(all_data)} dÃ²ng dá»¯ liá»‡u vÃ o {output_path}")
    return output_path


# ==========================
# ğŸ§  STEP 5: Cho phÃ©p test Ä‘á»™c láº­p
# ==========================
if __name__ == "__main__":
    topics = ["Vua Nguyá»…n HoÃ ng", 
              "Vua Gia Long",
              "Vua Minh Máº¡ng",
              "Vua Tá»± Äá»©c",
              "Tá»•ng tráº¥n Nguyá»…n VÄƒn TÆ°á»ng",
              "TÃ´n Tháº¥t Thuyáº¿t",
              "Vua HÃ m Nghi",
              "Thá»‘ng sá»© Trung Ká»³ Paul Doumer",
              "Phan Bá»™i ChÃ¢u",
              "CÆ°á»ng Äá»ƒ",
              "Phan ChÃ¢u Trinh",
              "Huá»³nh ThÃºc KhÃ¡ng",
              "Tráº§n QuÃ½ CÃ¡p",
              "Tráº§n Cao VÃ¢n",
              "ThÃ¡i PhiÃªn",
              "Vua Duy TÃ¢n",
              "Vua Báº£o Äáº¡i",
              "Chá»§ tá»‹ch Há»“ ChÃ­ Minh",
              "Tráº§n Huy Liá»‡u",
              "HoÃ  thÆ°á»£ng ThÃ­ch Quáº£ng Äá»©c",
              "ThÃ­ch TrÃ­ Quang",
              "NgÃ´ ÄÃ¬nh Diá»‡m",
              "TÆ°á»›ng NgÃ´ Quang TrÆ°á»Ÿng",
              "TÆ°á»›ng Tráº§n VÄƒn Háº£i",
              "VÃµ NguyÃªn GiÃ¡p",
              "TÃ­n Ä‘á»“ Ä‘áº¡o Máº«u ThiÃªn Y A Na",
              "Cá»™ng Ä‘á»“ng ngÆ°á»i Minh HÆ°Æ¡ng",
              "Triá»u Nguyá»…n",
              "CÃ´ng chÃºa Huyá»n TrÃ¢n",
              "NhÃ  Tráº§n",
              "Äá»©c Ã”ng Nam Háº£i"
              ]
    extract_people_data.fn(topics)
