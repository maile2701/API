#!/usr/bin/env python
# coding: utf-8

# 

# In[4]:


import requests
from bs4 import BeautifulSoup
import csv
import time
import pandas as pd
import re


# In[5]:


urls = [
    "https://vi.wikipedia.org/wiki/Ch%C3%B9a_Thi%C3%AAn_M%E1%BB%A5",
    "https://vi.wikipedia.org/wiki/C%E1%BB%91_%C4%91%C3%B4_Hu%E1%BA%BF",
    "https://vi.wikipedia.org/wiki/L%C4%83ng_Gia_Long",
    "https://vi.wikipedia.org/wiki/L%C4%83ng_Minh_M%E1%BA%A1ng",
    "https://vi.wikipedia.org/wiki/L%C4%83ng_T%E1%BB%B1_%C4%90%E1%BB%A9c",
    "https://vi.wikipedia.org/wiki/Tr%E1%BA%ADn_C%E1%BB%ADa_Thu%E1%BA%ADn_An",
    "https://vi.wikipedia.org/wiki/Phong_tr%C3%A0o_C%E1%BA%A7n_V%C6%B0%C6%A1ng",
    "https://vi.wikipedia.org/wiki/C%E1%BA%A7u_Tr%C6%B0%E1%BB%9Dng_Ti%E1%BB%81n",
    "https://vi.wikipedia.org/wiki/V%E1%BB%A5_m%C6%B0u_kh%E1%BB%9Fi_ngh%C4%A9a_%E1%BB%9F_Hu%E1%BA%BF_(1916)",
    "https://vi.wikipedia.org/wiki/S%E1%BB%B1_ki%E1%BB%87n_T%E1%BA%BFt_M%E1%BA%ADu_Th%C3%A2n",
    "https://vi.wikipedia.org/wiki/Festival_Hu%E1%BA%BF"
]
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
filepath = "/Users/thanhmai/etl_pipeline test/data/wiki_data_Hue.csv"

# In[6]:


with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["T√™n trang", "Thu·ªôc t√≠nh", "Gi√° tr·ªã"])


# In[7]:


def crawl_wiki(url):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    # --- T√™n trang ---
    title_tag = soup.find("h1", id="firstHeading")
    title = title_tag.text.strip() if title_tag else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"
    rows = []
    # --- Crawl Infobox ---
    infobox = soup.find("table", class_=lambda x: x and "infobox" in x)
    if infobox:
        for row in infobox.find_all("tr"):
            label = row.find("th")
            value = row.find("td")
            if label and value:
                key = label.get_text(" ", strip=True)
                val = value.get_text(" ", strip=True)
                rows.append([title, key, val])
    # --- Crawl ph·∫ßn "L·ªãch s·ª≠" ho·∫∑c "H√¨nh th√†nh" ---
    description = ""
    for header in soup.find_all(["h2", "h3"]):
        section_title = header.get_text(" ", strip=True).lower()
        if any(keyword in section_title for keyword in ["l·ªãch s·ª≠", "h√¨nh th√†nh", "kh·ªüi l·∫≠p", "qu√° tr√¨nh x√¢y d·ª±ng"]):
            content = []
            for sibling in header.find_next_siblings():
                if sibling.name in ["h2", "h3"]:
                    break
                if sibling.name == "p":
                    content.append(sibling.get_text(" ", strip=True))
            description = "\n".join(content).strip()
            break
    if not description and infobox:
        first_p_after_infobox = None
        # L·∫∑p qua c√°c th·∫ª sau infobox ƒë·ªÉ t√¨m th·∫ª <p> ƒë·∫ßu ti√™n
        for sibling in infobox.find_next_siblings():
            if sibling.name == 'p':
                first_p_after_infobox = sibling
                break # ƒê√£ t√¨m th·∫•y
        if first_p_after_infobox:
            description = first_p_after_infobox.get_text(" ", strip=True)
    if not description:
        first_p = soup.find("p")
        if first_p:
            description = first_p.get_text(" ", strip=True)
    # L√†m s·∫°ch k√Ω hi·ªáu [1], [2], [3]
    for j in range(1, 500):
        description = description.replace(f"[{j}]", "")
    # Ghi d·ªØ li·ªáu
    rows.append([title, "M√¥ t·∫£", description])
    with open(filepath, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerows(rows)
    print(f"‚úÖ ƒê√£ l∆∞u {title} ({len(rows)} d√≤ng).")


# In[8]:


for url in urls:
    try:
        crawl_wiki(url)
        time.sleep(2)
    except Exception as e:
        print(f"‚ùå L·ªói khi crawl {url}: {e}")

print(f"\nüéâ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u trong file: {filepath}")


# In[9]:


df = pd.read_csv('/Users/thanhmai/etl_pipeline test/data/wiki_data_Hue.csv')


# In[ ]:


DATE_ATTRS = [
    'Kh·ªüi l·∫≠p', 'X√¢y d·ª±ng', 'Ng√†y x√¢y d·ª±ng', 'Th·ªùi gian', 'Ho√†n th√†nh'
]
PERSON_ATTRS = [
    'Ng∆∞·ªùi s√°ng l·∫≠p', 'X√¢y d·ª±ng b·ªüi', 'X√¢y d·ª±ng\xa0b·ªüi',
    'Ng∆∞·ªùi x√¢y d·ª±ng', 'T·ªïng th·∫ßu'
]
LOCATION_ATTRS = [
    'Qu·ªëc gia', 'ƒê·ªãa ch·ªâ', 'V·ªã tr√≠', 'ƒê·ªãa ƒëi·ªÉm', 'B·∫Øc qua', 'T·ªça ƒë·ªô'
]
DESCRIPTION_ATTRS = [
    'M√¥ t·∫£','K·∫øt qu·∫£'
]

# --- 2. ƒê·ªåC FILE T·ª™ COLAB SESSION ---
file_name = '/Users/thanhmai/etl_pipeline test/data/wiki_data_Hue.csv'

try:
    df_raw = pd.read_csv(file_name)
    print(f"--- ƒê√£ ƒë·ªçc file '{file_name}' th√†nh c√¥ng ---")

    # --- 3. ƒê·ªäNH NGHƒ®A H√ÄM BI·∫æN ƒê·ªîI CHU·∫®N H√ìA (ƒê√£ c·∫≠p nh·∫≠t) ---
    def transform_group(group):
        record = {
            'event_name': group['T√™n trang'].iloc[0],
            'description': [],
            'event_date': None,
            'event_location': [],
            'person': None
        }

        for _, row in group.iterrows():
            if 'Thu·ªôc t√≠nh' not in row or 'Gi√° tr·ªã' not in row:
                continue

            attr = str(row['Thu·ªôc t√≠nh'])
            val = str(row['Gi√° tr·ªã'])
            if attr in DATE_ATTRS:
                match = re.match(r'(\d+)\s+(.*)', val)
                if match:
                    record['event_date'] = match.group(1) # '1802'
                    record['event_location'].append(match.group(2)) # 'Ph√∫ Xu√¢n'
                else:
                    record['event_date'] = val

            elif attr in PERSON_ATTRS:
                record['person'] = val

            elif attr in LOCATION_ATTRS:
                record['event_location'].append(val)

            elif attr in DESCRIPTION_ATTRS:
                record['description'].append(f"{attr}: {val}")

        record['description'] = ". ".join(record['description'])
        record['event_location'] = ", ".join(record['event_location'])

        return pd.Series(record)

    # --- 4. √ÅP D·ª§NG H√ÄM BI·∫æN ƒê·ªîI ---
    if 'T√™n trang' not in df_raw.columns:
        print("L·ªñI: File c·ªßa b·∫°n kh√¥ng c√≥ c·ªôt 'T√™n trang'.")
        raise ValueError("Missing required column: 'T√™n trang'")

    df_clean = df_raw.groupby('T√™n trang').apply(transform_group).reset_index(drop=True)

    print("\n" + "="*30 + "\n")
    print("--- D·ªÆ LI·ªÜU ƒê√É CHU·∫®N H√ìA (HO√ÄN CH·ªàNH) ---")
    print(df_clean)

    # --- 5. L∆ØU FILE K·∫æT QU·∫¢ ---
    output_file = "/Users/thanhmai/etl_pipeline test/data/Wiki_Hue_chuyendoi.csv"
    df_clean.to_csv(output_file, index=False)
    print(f"\nƒê√£ l∆∞u file chu·∫©n h√≥a t·∫°i: {output_file}")
except FileNotFoundError:
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file t√™n l√† '{file_name}'")
except Exception as e:
    print(f"ƒê√£ x·∫£y ra l·ªói khi ƒë·ªçc ho·∫∑c x·ª≠ l√Ω file: {e}")


# C√°c l·ªÖ h·ªôi kh√°c kh√¥ng c√≥ tr√™n wiki

# In[ ]:


event_links = [
    "https://khamphahue.com.vn/Van-hoa/Chi-tiet/tid/Le-hoi-Du-Tien.html/pid/14959/cid/198",
    "https://khamphahue.com.vn/Van-hoa/Chi-tiet/tid/Le-Hoi-Dien-Hon-Chen.html/pid/3036/cid/198",
    "https://khamphahue.com.vn/Van-hoa/Chi-tiet/cid/198/pid/5780",
    "https://khamphahue.com.vn/Van-hoa/Chi-tiet/tid/Le-hoi-den-Huyen-Tran-Cong-chua.html/pid/1396/cid/198"
]


# In[ ]:


def crawl_event_detail(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi t·∫£i {url}: {e}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # --- T√™n s·ª± ki·ªán ---
    title = soup.find("h1").text.strip() if soup.find("h1") else "Kh√¥ng r√µ"

    # --- Th·ªùi gian ---
    time_tag = soup.find("span", id="dnn_ctr1099_ViewTinBai_DsChuyenMucTinBai_lblTableThoiGian")
    event_date = time_tag.text.strip() if time_tag else "Kh√¥ng c√≥ th√¥ng tin th·ªùi gian"

    # --- ƒê·ªãa ch·ªâ ---
    address_tag = soup.find("span", id="dnn_ctr1099_ViewTinBai_DsChuyenMucTinBai_lblTableDiaChi")
    event_location = address_tag.text.strip() if address_tag else "Kh√¥ng c√≥ th√¥ng tin ƒë·ªãa ch·ªâ"

    # --- M√î T·∫¢:
    desc_box = soup.find("div", id="dnn_ctr1099_ViewTinBai_DsChuyenMucTinBai_groupGioiThieu")
    content_box = soup.find("div", class_="content-text-top")

    description = ""
    if desc_box:
        paragraphs = [p.get_text(" ", strip=True) for p in desc_box.find_all("p")]
        description = "\n".join(paragraphs)
    elif content_box:
        paragraphs = [p.get_text(" ", strip=True) for p in content_box.find_all("p")]
        description = "\n".join(paragraphs)
    else:
        description = "Kh√¥ng c√≥ m√¥ t·∫£"

    return {
        "event_name": title,
        "description": description,
        "event_date": event_date,
        "event_location": event_location,
        "url": url
    }


# In[ ]:


all_events = []
for i, link in enumerate(event_links, start=1):
    print(f"({i}/{len(event_links)}) ƒêang crawl: {link}")
    data = crawl_event_detail(link)
    if data:
        all_events.append(data)
    time.sleep(2)


# In[ ]:


import pandas as pd

df = pd.DataFrame(all_events)
df.to_csv("/Users/thanhmai/etl_pipeline test/data/hue_festivals.csv", index=False, encoding="utf-8-sig")

print("‚úÖ ƒê√£ crawl xong v√† l∆∞u v√†o hue_festivals.csv")
print(df.head())


# event ƒê√† N·∫µng

# In[ ]:


urls = [
    "https://vi.wikipedia.org/wiki/Tr%E1%BA%ADn_%C4%90%C3%A0_N%E1%BA%B5ng_(1859%E2%80%931860)",
    "https://vi.wikipedia.org/wiki/Chi%E1%BA%BFn_d%E1%BB%8Bch_Hu%E1%BA%BF_%E2%80%93_%C4%90%C3%A0_N%E1%BA%B5ng",
    "https://vi.wikipedia.org/wiki/B%C3%A0_N%C3%A0",
    "https://vi.wikipedia.org/wiki/Ch%C3%B9a_Linh_%E1%BB%A8ng",
    "https://vi.wikipedia.org/wiki/C%E1%BA%A7u_S%C3%B4ng_H%C3%A0n",
    "https://vi.wikipedia.org/wiki/C%E1%BA%A7u_Tr%E1%BA%A7n_Th%E1%BB%8B_L%C3%BD",
    "https://vi.wikipedia.org/wiki/C%E1%BA%A7u_R%E1%BB%93ng",
    "https://vi.wikipedia.org/wiki/C%E1%BA%A7u_Thu%E1%BA%ADn_Ph%C6%B0%E1%BB%9Bc"
]
headers = {"User-Agent": "Mozilla/5.0"}
filepath = "/Users/thanhmai/etl_pipeline test/data/wikipedia_data_DN.csv"


# In[ ]:


with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["T√™n trang", "Thu·ªôc t√≠nh", "Gi√° tr·ªã"])


# In[ ]:


def crawl_wiki(url):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    title_tag = soup.find("h1", id="firstHeading")
    title = title_tag.text.strip() if title_tag else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"
    rows = []
    # --- Crawl Infobox ---
    infobox = soup.find("table", class_=lambda x: x and "infobox" in x)
    if infobox:
        for row in infobox.find_all("tr"):
            label = row.find("th")
            value = row.find("td")
            if label and value:
                key = label.get_text(" ", strip=True)
                val = value.get_text(" ", strip=True)
                rows.append([title, key, val])
    # --- Crawl ph·∫ßn "L·ªãch s·ª≠" ho·∫∑c "H√¨nh th√†nh" ---
    description = ""
    for header in soup.find_all(["h2", "h3"]):
        section_title = header.get_text(" ", strip=True).lower()
        if any(keyword in section_title for keyword in ["l·ªãch s·ª≠", "h√¨nh th√†nh", "kh·ªüi l·∫≠p", "qu√° tr√¨nh x√¢y d·ª±ng"]):
            content = []
            for sibling in header.find_next_siblings():
                if sibling.name in ["h2", "h3"]:
                    break
                if sibling.name == "p":
                    content.append(sibling.get_text(" ", strip=True))
            description = "\n".join(content).strip()
            break
    if not description and infobox:
        first_p_after_infobox = None
        # L·∫∑p qua c√°c th·∫ª sau infobox ƒë·ªÉ t√¨m th·∫ª <p> ƒë·∫ßu ti√™n
        for sibling in infobox.find_next_siblings():
            if sibling.name == 'p':
                first_p_after_infobox = sibling
                break
        if first_p_after_infobox:
            description = first_p_after_infobox.get_text(" ", strip=True)
    if not description:
        first_p = soup.find("p")
        if first_p:
            description = first_p.get_text(" ", strip=True)

    for j in range(1, 500):
        description = description.replace(f"[{j}]", "")
    # Ghi d·ªØ li·ªáu
    rows.append([title, "M√¥ t·∫£", description])
    with open(filepath, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerows(rows)
    print(f"‚úÖ ƒê√£ l∆∞u {title} ({len(rows)} d√≤ng).")


# In[ ]:


for url in urls:
    try:
        crawl_wiki(url)
        time.sleep(2)
    except Exception as e:
        print(f"‚ùå L·ªói khi crawl {url}: {e}")

print(f"\nüéâ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u trong file: {filepath}")


# In[ ]:


import pandas as pd
import re

# === 1. ƒê·ªçc file g·ªëc ===
file_path = "/Users/thanhmai/etl_pipeline test/data/wikipedia_data_DN.csv"
df = pd.read_csv(file_path)
print("‚úÖ ƒê·ªçc file th√†nh c√¥ng! S·ªë d√≤ng:", len(df))

# === 2. ƒê·ªãnh nghƒ©a nh√≥m thu·ªôc t√≠nh ===
DATE_START_ATTRS = ['Th·ªùi gian kh·ªüi c√¥ng', 'Kh·ªüi c√¥ng']
DATE_END_ATTRS = ['Th·ªùi gian ho√†n th√†nh', 'Ho√†n th√†nh', 'Th√¥ng xe']
LOCATION_ATTRS = ['ƒê·ªãa ƒëi·ªÉm', 'V·ªã tr√≠', 'B·∫Øc qua', 'T·ªça ƒë·ªô']
PERSON_ATTRS = ['Ng∆∞·ªùi thi·∫øt k·∫ø', 'Ng∆∞·ªùi s√°ng l·∫≠p', 'X√¢y d·ª±ng b·ªüi', 'T·ªïng th·∫ßu']
DESCRIPTION_ATTRS = ['M√¥ t·∫£', 'K·∫øt qu·∫£', 'Th√¥ng tin']

def normalize_date(text):
    text = str(text).strip()
    text = re.sub(r'(\d{1,2})\s*th√°ng\s*(\d{1,2})\s*nƒÉm\s*(\d{4})', r'\1/\2/\3', text)
    text = re.sub(r'(\d{1,2})\s*/\s*(\d{1,2})\s*/\s*(\d{4})', r'\1/\2/\3', text)
    return text if text else None

def transform_group(group):
    record = {
        'event_name': group['T√™n trang'].iloc[0],
        'description': [],
        'event_date_start': None,
        'event_date_end': None,
        'event_location': [],
        'person': None
    }

    for _, row in group.iterrows():
        attr = str(row['Thu·ªôc t√≠nh']).strip()
        val = str(row['Gi√° tr·ªã']).strip()

        val_normalized = normalize_date(val)

        if attr in DATE_START_ATTRS and val_normalized:
            record['event_date_start'] = val_normalized
        elif attr in DATE_END_ATTRS and val_normalized:
            record['event_date_end'] = val_normalized
        elif attr in LOCATION_ATTRS:
            record['event_location'].append(val)
        elif attr in PERSON_ATTRS:
            record['person'] = val
        elif attr in DESCRIPTION_ATTRS:
            val = re.sub(r'^(M√¥ t·∫£|K·∫øt qu·∫£|Th√¥ng tin)\s*:\s*', '', val)
            record['description'].append(val)

    record['event_location'] = ", ".join(record['event_location'])
    record['description'] = ". ".join(record['description'])

    # G·ªôp ng√†y kh·ªüi c√¥ng v√† ho√†n th√†nh
    if record['event_date_start'] and record['event_date_end']:
        record['event_date'] = f"{record['event_date_start']} - {record['event_date_end']}"
    elif record['event_date_start']:
        record['event_date'] = record['event_date_start']
    elif record['event_date_end']:
        record['event_date'] = record['event_date_end']
    else:
        record['event_date'] = None

    return pd.Series({
        'event_name': record['event_name'],
        'description': record['description'],
        'event_date': record['event_date'],
        'event_location': record['event_location'],
        'person': record['person']
    })

# === 5. Gom nh√≥m theo T√™n trang ===
df_clean = df.groupby('T√™n trang').apply(transform_group).reset_index(drop=True)

# === 6. Xu·∫•t k·∫øt qu·∫£ ===
output_file = "/Users/thanhmai/etl_pipeline test/data/wikipedia_data_DN_clean.csv"
df_clean.to_csv(output_file, index=False, encoding='utf-8-sig')

print(f"\n‚úÖ ƒê√£ l∆∞u file chu·∫©n h√≥a t·∫°i: {output_file}")
print(df_clean.head(10))


# In[ ]:


import pandas as pd

# 1. ƒê·ªçc 2 file (ho·∫∑c nhi·ªÅu h∆°n)
df1 = pd.read_csv(f"/Users/thanhmai/etl_pipeline test/data/Wiki_Hue_chuyendoi.csv")
df2 = pd.read_csv("/Users/thanhmai/etl_pipeline test/data/hue_festivals.csv")
df3 = pd.read_csv("/Users/thanhmai/etl_pipeline test/data/wikipedia_data_DN_clean.csv")

# 2. G·ªôp ch√∫ng l·∫°i th√†nh m·ªôt danh s√°ch
danh_sach_dfs = [df1, df2,df3]

# 3. D√πng pd.concat ƒë·ªÉ n·ªëi
event_cap2 = pd.concat(danh_sach_dfs, ignore_index=True)
print(event_cap2)

# 5. L∆∞u file
event_cap2.to_csv("/Users/thanhmai/etl_pipeline test/data/event_raw.csv", index=False)

